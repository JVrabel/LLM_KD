model:
  name: "meta-llama/Llama-3.2-3B"
  use_gradient_checkpointing: true

data:
  sources:
    - path: "data/test/output_phil/pretraining.jsonl"
      max_tokens: 1000000  # 1M tokens
    - path: "data/test/output/pretraining.jsonl"
      max_tokens: 500000   # 500K tokens
    # Can add more sources with their token limits
  max_length: 512
  stride: 380
  batch_size: 32
  num_workers: 6

training:
  learning_rate: 1e-4
  epochs: 100
  max_steps_per_epoch: null
  gradient_accumulation_steps: 1
  clip_grad_norm: 1.0
  kd_ratio: 1.0
  seed: 42
  log_every_n_steps: 100
  eval_every: 200
  eval_steps: 200

checkpointing:
  save_best: true
  save_last: true
  save_every_n_epochs: 5  # Save every 5 epochs
  keep_n_checkpoints: 2
  resume: null  # Path to checkpoint to resume from, null if starting fresh

output:
  dir: "runs/kd_experiment_3b"

  wandb:
  enabled: true
  project: "llm-kd"
  name: null  # Will be auto-generated if null
  tags: ["3B", "knowledge-distillation"]
  notes: "Training 3B student with KD"