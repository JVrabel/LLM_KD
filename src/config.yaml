model:
  name: "meta-llama/Llama-2-3b"  # or whichever LLaMA model you're using
  use_gradient_checkpointing: true
  mixed_precision: "bf16"  # or "fp16" if you want to use AMP

data:
  sources:
    - path: "data/gutenberg/train/bookshelf_57_train.jsonl"
      max_tokens: 20000000  # Adjust based on your needs
  max_length: 512
  stride: 380
  batch_size: 32
  num_workers: 6
  test_path: "data/gutenberg/test/bookshelf_57_test.jsonl"

training:
  learning_rate: 1.0e-4
  epochs: 100
  max_steps_per_epoch: null
  gradient_accumulation_steps: 2
  clip_grad_norm: 1.0
  kd_ratio: 1.0
  seed: 42
  log_every_n_steps: 100
  eval_every: 200
  eval_steps: 200

checkpointing:
  save_best: true
  save_last: true
  save_every_n_epochs: 5
  keep_n_checkpoints: 1
  resume: null

output:
  dir: "runs/kd_experiment_3b"

wandb:
  enabled: true
  project: "llm-kd"
  name: null
  tags: ["1B", "knowledge-distillation"]
  notes: "Training 1B student from 1B teacher with KD"
  log_every_n_steps: 10
  generate_every_n_steps: 500  # Generate samples every 500 steps


  #python src/modularize_phil_both_datasets.py --config config.yaml --resume runs/kd_experiment_3b/run_20240101_123456/checkpoints/best_model.pt