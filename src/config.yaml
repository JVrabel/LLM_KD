model:
  name: "meta-llama/Llama-3.2-1b"
  student:
    reduce_size: true  # Whether to reduce student model size
    size_reduction_factor: 2  # Divide hidden layers and intermediate size by this factor

data:
  sources:
    - path: "data/gutenberg/train/bookshelf_57_train.jsonl"
  max_length: 512
  stride: 380
  batch_size: 4

training:
  learning_rate: 1.0e-4
  epochs: 100
  max_steps_per_epoch: null
  gradient_accumulation_steps: 4
  clip_grad_norm: 1.0
  kd_ratio: 1.0
  seed: 42
  log_every_n_steps: 100
  eval_every: 200
  eval_steps: 200
  num_gpus: 1  # Set to 1 for single GPU
  distributed:
    enabled: false  # Disable distributed training
    backend: "nccl"

checkpointing:
  save_every_n_epochs: 5
  keep_n_checkpoints: 1
  resume: null

output:
  dir: "runs/kd_LLM_modular_v1"

wandb:
  enabled: true
  project: "llm-kd"
  name: null
  tags: ["1B", "knowledge-distillation"]
  notes: "Training 1B student from 1B teacher with KD"
  log_every_n_steps: 10
  generate_every_n_steps: 500  # Generate samples every 500 steps


  #python src/modularize_phil_both_datasets.py --config config.yaml --resume runs/kd_experiment_3b/run_20240101_123456/checkpoints/best_model.pt